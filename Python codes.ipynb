{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"imports\"></a>1. Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# text processing libraries\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "# sklearn \n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"reading\"></a> 2. Reading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files available\n",
    "print(os.listdir(\"../input/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Training data\n",
    "train = pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "print('Training data shape: ', train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data has 7613 observations and 5 features including the TARGET (the label we want to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data \n",
    "test = pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "print('Testing data shape: ', test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the test data doesn't have the target column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"eda\"></a> 3. Basic EDA\n",
    "\n",
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing values in training set\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns denote the following: \n",
    "\n",
    "- The `text` of a tweet\n",
    "- A `keyword` from that tweet\n",
    "- The `location` the tweet was sent from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing values in test set\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of values are missing in the `location` column in both the training and the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Target Column\n",
    "\n",
    "* ** Distribution of the Target Column**\n",
    "\n",
    "We have to predict whether a given tweet is about a real disaster or not. - If so, predict a 1. If not, predict a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(train['target'].value_counts().index,train['target'].value_counts(),palette='rocket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Exploring the Target Column**\n",
    "Let's look at what the disaster and the non disaster tweets look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A disaster tweet\n",
    "disaster_tweets = train[train['target']==1]['text']\n",
    "disaster_tweets.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not a disaster tweet\n",
    "non_disaster_tweets = train[train['target']==0]['text']\n",
    "non_disaster_tweets.values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the 'keyword' column\n",
    "The keyword column denotes a keyword from the tweet.Let's look at the top 20 keywords in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y=train['keyword'].value_counts()[:20].index,x=train['keyword'].value_counts()[:20],\n",
    "            orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how often the word 'disaster' come in the dataset and whether this help us in determining whether a tweet belongs to a disaster category or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['text'].str.contains('disaster', na=False, case=False)].target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the 'location' column\n",
    "Even though the column `location` has a number of missing values, let's see the top 20 locations present in the dataset. Since some of the locations are repeated, this will require some bit of cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Replacing the ambigious locations name with Standard names\n",
    "train['location'].replace({'United States':'USA',\n",
    "                           'New York':'USA',\n",
    "                            \"London\":'UK',\n",
    "                            \"Los Angeles, CA\":'USA',\n",
    "                            \"Washington, D.C.\":'USA',\n",
    "                            \"California\":'USA',\n",
    "                             \"Chicago, IL\":'USA',\n",
    "                             \"Chicago\":'USA',\n",
    "                            \"New York, NY\":'USA',\n",
    "                            \"California, USA\":'USA',\n",
    "                            \"FLorida\":'USA',\n",
    "                            \"Nigeria\":'Africa',\n",
    "                            \"Kenya\":'Africa',\n",
    "                            \"Everywhere\":'Worldwide',\n",
    "                            \"San Francisco\":'USA',\n",
    "                            \"Florida\":'USA',\n",
    "                            \"United Kingdom\":'UK',\n",
    "                            \"Los Angeles\":'USA',\n",
    "                            \"Toronto\":'Canada',\n",
    "                            \"San Francisco, CA\":'USA',\n",
    "                            \"NYC\":'USA',\n",
    "                            \"Seattle\":'USA',\n",
    "                            \"Earth\":'Worldwide',\n",
    "                            \"Ireland\":'UK',\n",
    "                            \"London, England\":'UK',\n",
    "                            \"New York City\":'USA',\n",
    "                            \"Texas\":'USA',\n",
    "                            \"London, UK\":'UK',\n",
    "                            \"Atlanta, GA\":'USA',\n",
    "                            \"Mumbai\":\"India\"},inplace=True)\n",
    "\n",
    "sns.barplot(y=train['location'].value_counts()[:5].index,x=train['location'].value_counts()[:5],\n",
    "            orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"processing\"></a> 4. Text Data Preprocessing\n",
    "\n",
    "## 1. Data Cleaning\n",
    "\n",
    "Before we start with any NLP project we need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick glance over the existing data\n",
    "train['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying a first round of text cleaning techniques\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "# Applying the cleaning function to both test and training datasets\n",
    "train['text'] = train['text'].apply(lambda x: clean_text(x))\n",
    "test['text'] = test['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "train['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun let's create a wordcloud of the clean text to see the most dominating words in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\n",
    "wordcloud1 = WordCloud( background_color='white',\n",
    "                        width=600,\n",
    "                        height=400).generate(\" \".join(disaster_tweets))\n",
    "ax1.imshow(wordcloud1)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Disaster Tweets',fontsize=40);\n",
    "\n",
    "wordcloud2 = WordCloud( background_color='white',\n",
    "                        width=600,\n",
    "                        height=400).generate(\" \".join(non_disaster_tweets))\n",
    "ax2.imshow(wordcloud2)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Non Disaster Tweets',fontsize=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "Tokenization is a process that splits an input sequence into so-called tokens where the tokens can be a word, sentence, paragraph etc. Base upon the type of tokens we want, tokenization can be of various types, for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Are you coming , aren't you\"\n",
    "tokenizer1 = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer2 = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer3 = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "print(\"Example Text: \",text)\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "print(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\n",
    "print(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\n",
    "print(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\n",
    "print(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the training and the test set\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "train['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "test['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "train['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stopwords Removal\n",
    "\n",
    "Now, let's get rid of the stopwords i.e words which occur very frequently but have no possible value like **a, an, the, are **etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removing stopwords belonging to english language\n",
    "    \n",
    "    \"\"\"\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x : remove_stopwords(x))\n",
    "test['text'] = test['text'].apply(lambda x : remove_stopwords(x))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Token normalization\n",
    "\n",
    "Token normalisation means converting different tokens to their base forms. This can be done either by:\n",
    "\n",
    "- **Stemming** :  removing and replacing suffixes to get to the root form of the word, which is called the **stem** for instance cats - cat, wolves - wolv \n",
    "- **Lemmatization** : Returns the base or dictionary form of a word, which is known as the **lemma** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and Lemmatization examples\n",
    "text = \"feet cats wolves talked\"\n",
    "\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "print(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n",
    "\n",
    "# Lemmatizer\n",
    "lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "print(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After preprocessing, the text format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x : combine_text(x))\n",
    "test['text'] = test['text'].apply(lambda x : combine_text(x))\n",
    "train['text']\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting it all together- A Text Preprocessing Function\n",
    "This concludes the pre-processing part. It will be prudent to convert all the steps undertaken into a function for better reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing function\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Cleaning and parsing the text.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    nopunc = clean_text(text)\n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n",
    "    combined_text = ' '.join(remove_stopwords)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"vectorization\"></a>  5. Transforming tokens to a vector\n",
    "After the initial preprocessing phase, we need to transform text into a meaningful vector (or array) of numbers. This can be done by a number of tecniques:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(train['text'])\n",
    "test_vectors = count_vectorizer.transform(test[\"text\"])\n",
    "\n",
    "## Keeping only non-zero elements to preserve space \n",
    "print(train_vectors[0].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Features\n",
    "\n",
    "A problem with the Bag of Words approach is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much “informational content”. Also, it will give more weight to longer documents than shorter documents.\n",
    "\n",
    "One approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like “the” that are also frequent across all documents are penalized. This approach to scoring is called Term Frequency-Inverse Document Frequency, or TF-IDF for short, where:\n",
    "\n",
    "**Term Frequency: is a scoring of the frequency of the word in the current document.**\n",
    "\n",
    "```\n",
    "TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "```\n",
    "\n",
    "**Inverse Document Frequency: is a scoring of how rare the word is across documents.**\n",
    "\n",
    "```\n",
    "IDF = 1+log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "train_tfidf = tfidf.fit_transform(train['text'])\n",
    "test_tfidf = tfidf.transform(test[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"model\"></a> 6. Building a Text Classification model\n",
    "Now the data is ready to be fed into a classification model. Let's create a basic claasification model using commonly used classification algorithms and see how our model performs.\n",
    "\n",
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression(C=1.0)\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_vectors, train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf_tfidf = LogisticRegression(C=1.0)\n",
    "scores = model_selection.cross_val_score(clf_tfidf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the countvectorizer gives a better performance than TFIDF in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naives Bayes Classifier\n",
    "Well, this is a decent score. Let's try with another model that is said to work well with text data : Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple Naive Bayes on Counts\n",
    "clf_NB = MultinomialNB()\n",
    "scores = model_selection.cross_val_score(clf_NB, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_NB.fit(train_vectors, train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf_NB_TFIDF = MultinomialNB()\n",
    "scores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "well the naive bayes on TFIDF features scores much better than logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_NB_TFIDF.fit(train_tfidf, train[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "clf_xgb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "scores = model_selection.cross_val_score(clf_xgb, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "clf_xgb_TFIDF = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "scores = model_selection.cross_val_score(clf_xgb_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(submission_file_path,model,test_vectors):\n",
    "    sample_submission = pd.read_csv(submission_file_path)\n",
    "    sample_submission[\"target\"] = model.predict(test_vectors)\n",
    "    sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file_path = \"../input/nlp-getting-started/sample_submission.csv\"\n",
    "test_vectors=test_tfidf\n",
    "submission(submission_file_path,clf_NB_TFIDF,test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "\n",
    "FileLink('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
